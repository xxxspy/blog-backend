
---
title: 最全中文词向量数据下载-都是训练好的优质向量
date: 2018-06-28 18:17:55
tags: [词向量]
toc: true
xiongzhang: true
xiongzhang_images: [main.jpg]

---
<span></span>

词向量是进行自然语言处理的常用技术, 但是训练词向量需要很多语料库和计算力, 于是有人训练好了很多词向量供我们使用, 下面是一些优质的词向量, 可以直接下载使用。

<!-- more -->

# 中文词向量

以下文字由本站站长翻译而来:[原文在此](https://github.com/Embedding)

这个项目提供100多个用不同**表示**（密集和稀疏），**上下文特征**（单词，ngram，字符等）和**语料库**训练的中文单词向量（嵌入）。可以轻松获得具有不同属性的预先训练的向量，并将它们用于下游任务。

此外，我们提供了一个中文类比推理数据集** CA8 **和评估工具包，供用户评估其词向量的质量。



## 引用

请引用论文，如果使用这些嵌入和CA8数据集。

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href="https://arxiv.org/abs/1805.06504"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, accepted by ACL 2018.

```
@article{shen2018analogical,
  title={Analogical Reasoning on Chinese Morphological and Semantic Relations},
  author={Shen, Li and Zhe, Zhao and Renfen, Hu and Wensi, Li and Tao, Liu and Xiaoyong, Du},
  journal={arXiv preprint arXiv:1805.06504},
  year={2018}
}
```

```
@InProceedings{shen2018analogical,
  title={Analogical Reasoning on Chinese Morphological and Semantic Relations},
  author={Shen, Li and Zhe, Zhao and Renfen, Hu and Wensi, Li and Tao, Liu and Xiaoyong, Du},
  year={2018},
}
```

## 格式

预先训练好的向量文件是文本格式。每行包含一个单词和它的向量。每个值由空格分隔。第一行记录元信息：第一个数字表示文件中的字数，第二个数字表示向量大小。

除了密集的单词向量（用SGNS训练）之外，我们还提供稀疏向量（用PPMI训练）。它们与liblinear的格式相同，其中“：”之前的数字表示维度索引，“：”之后的数字表示该值。


## 预训练的中文单词矢量

### 基本设置

<table>  <tr align="center">    <td><b>Window Size</b></td>    <td><b>Dynamic Window</b></td>    <td><b>Sub-sampling</b></td>    <td><b>Low-Frequency Word</b></td>    <td><b>Iteration</b></td>    <td><b>Negative Sampling<sup>*</sup></b></td>  </tr>  <tr align="center">    <td>5</td>    <td>Yes</td>    <td>1e-5</td>    <td>10</td>    <td>5</td>    <td>5</td>  </tr></table>

### 各种领域词向量

用不同的表示法，上下文特征和语料库训练的中文单词向量。下面的下载链接都是我的百度网盘, 因为怕资料丢失, 所以统一保存在了我的网盘, 后期可能回不时更新。

<table align="center">    <tr align="center">        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>    </tr>    <tr align="center">        <td rowspan="2">Corpus</td>        <td colspan="4">Context Features</td>    </tr>    <tr  align="center">      <td>Word</td>      <td>Word + Ngram</td>      <td>Word + Character</td>      <td>Word + Character + Ngram</td>    </tr>    <tr  align="center">      <td>Baidu Encyclopedia 百度百科</td>      <td><a href="https://pan.baidu.com/s/1IZ_OUpEvaWB5yzrF9cw3tw">300d</a></td>      <td><a href="https://pan.baidu.com/s/14QdVNpmAQMCY-Mt40l7Mvw">300d</a></td>      <td><a href="https://pan.baidu.com/s/1xirKzGJcDOIOO69uhavYeQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1y61p6nTioxg06mQSkFiO8g">300d</a></td>    </tr>    <tr  align="center">      <td>Wikipedia_zh 中文维基百科</td>      <td><a href="https://pan.baidu.com/s/1RjQMtXYZzre-JYBZ1t2QNg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1Mu-5C8UMqNYjR9QyrTwLOQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1Wgq5kP4Wo99GA-sHhjyCIA">300d</a></td>      <td><a href="https://pan.baidu.com/s/19s419DFZ2ZghF_1aiGTTVA">300d</td>    </tr>    <tr  align="center">      <td>People's Daily News 人民日报</td>      <td><a href="https://pan.baidu.com/s/16OaePQrj0Igk_S_8eMGcZg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1I4NIssIp1UEL5ofTWGKp-A">300d</a></td>      <td><a href="https://pan.baidu.com/s/1-g0wFU5eC3L-Fy05BmpQDA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1STYHdLYtGYhaZb_hwQHWJA">300d</a></td>    </tr>    <tr  align="center">      <td>Sogou News 搜狗新闻</td>      <td><a href="https://pan.baidu.com/s/1TUDBtmrtFGZ5Dtb18qRePg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1aLEJ-0wrdqL2I--3cKlA4A">300d</a></td>      <td><a href="https://pan.baidu.com/s/1LRCu8Se39CYechdeBhretA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1OjGpfuSEdwghVCFAHVmavA">300d</a></td>    </tr>    <tr  align="center">      <td>Financial News 金融新闻</td>      <td><a href="https://pan.baidu.com/s/1EDJCozH2P9pd7xX2h8Yh9g">300d</a></td>      <td><a href="https://pan.baidu.com/s/1laX53qDYjJDXnjpAcaQ9dA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1p_EPSu38yW3eofICuSV24Q">300d</a></td>      <td><a href="https://pan.baidu.com/s/1S8MgMxGht_pNvxM8b1Jmnw">300d</a></td>    </tr>    <tr  align="center">      <td>Zhihu_QA 知乎问答 </td>      <td><a href="https://pan.baidu.com/s/1zNsVGvKsHUzwZUTnBVPEJw">300d</a></td>      <td><a href="https://pan.baidu.com/s/1yA7gYWJNwjBy8_cyacpIbA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1AOInq8DyW5s523sw5qqmaQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/17Jxte8sSmtJynD268TMY8g">300d</a></td>    </tr>    <tr  align="center">      <td>Weibo 微博</td>      <td><a href="https://pan.baidu.com/s/1TuwvWovPtvG7yjhk_i8U5g">300d</a></td>      <td><a href="https://pan.baidu.com/s/1H-s9I5QN454UhXrywHDyrQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1qyVRnN_-186e_-oKpf9zCg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1RxwXGi4f_WSVom7Jha59cQ">300d</a></td>    </tr>    <tr  align="center">      <td>Literature 文学作品</td>      <td><a href="https://pan.baidu.com/s/1zQkveTO-wdV8kdRomtXypw">300d</a></td>      <td><a href="https://pan.baidu.com/s/1tue5-ZsqMq0zRKgF_hND1g">300d</a></td>      <td><a href="https://pan.baidu.com/s/1mxF9wgqATQDYucoWC7SdeQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1fWw23ezXrEH8paCTrkbdGA">300d</a></td>    </tr>    <tr  align="center">      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>      <td><a href="https://pan.baidu.com/s/15y3hZbbfyJERWPz6vRiaww">300d</a></td>      <td><a href="https://pan.baidu.com/s/1ahfizoYbXzT-3lOyRv5E6A">300d</a></td>      <td>NAN</td>      <td>NAN</td>    </tr>    <tr  align="center">      <td>Mixed-large 综合</td>      <td><a href="https://pan.baidu.com/s/1VX0G5yna4VpQpjV1IgYTMQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1Zu5t8f0R3eZIE5g_YHoxTQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1_Hm3Z0Ftmd-hHe9ukFMqSA">300d</a></td>      <td>300d</td>    </tr></table>
<table align="center">    <tr align="center">        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>    </tr>    <tr align="center">        <td rowspan="2">Corpus</td>        <td colspan="4">Context Features</td>    </tr>    <tr  align="center">      <td>Word</td>      <td>Word + Ngram</td>      <td>Word + Character</td>      <td>Word + Character + Ngram</td>    </tr>    <tr  align="center">      <td>Baidu Encyclopedia 百度百科</td>      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1sDvRiyp9tCN6qWQzNmkJ-g">300d</a></td>      <td><a href="https://pan.baidu.com/s/17cyahL-s9zf4Pn3Y9azhqQ">300d</a></td>    </tr>    <tr  align="center">      <td>Wikipedia_zh 中文维基百科</td>      <td><a href="https://pan.baidu.com/s/1sPCjyFKjWhftMIbiNQrL5g">300d</a></td>      <td><a href="https://pan.baidu.com/s/1N1JwPVuIMFUz9mVpOTbGPA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1lShfPail-J-sMDfZO_kesw">300d</a></td>      <td><a href="https://pan.baidu.com/s/1QQ8vszOUtqG9giK5idyDfg">300d</a></td>    </tr>    <tr  align="center">      <td>People's Daily News 人民日报</td>      <td><a href="https://pan.baidu.com/s/1o5DokVdd2pCcGZ3SXVL2tA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1oq44HlD-h31y43CD_TCmew">300d</a></td>      <td><a href="https://pan.baidu.com/s/1QLuv4oG75dMDmEqOieWAcg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1vHpJHxYroQGMSsMAGhUp1Q">300d</a></td>    </tr>    <tr  align="center">      <td>Sogou News 搜狗新闻</td>      <td><a href="https://pan.baidu.com/s/1GK5p0oc365ixj-6EoKpk8Q">300d</a></td>      <td><a href="https://pan.baidu.com/s/1qHriZ98O4bzByULkdrxq-A">300d</a></td>      <td><a href="https://pan.baidu.com/s/17j2M17thLHWh7Z9-GzrrPg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1m_eUJ1sTjQg9qAZhdPptqw">300d</a></td>    </tr>    <tr  align="center">      <td>Financial News 金融新闻</td>      <td><a href="https://pan.baidu.com/s/15gAra4FqXweJKRLOXWwMSg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1ZL3lXQ8A_3W-efZAB5I72A">300d</a></td>      <td><a href="https://pan.baidu.com/s/10eY_p9ZyHd9YG9xJWZh5JQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/13LYceys1w3_ztU0oA7YauQ">300d</a></td>    </tr>    <tr  align="center">      <td>Zhihu_QA 知乎问答 </td>      <td><a href="https://pan.baidu.com/s/15oVsIlriZlmLiPTdOyKzvA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1gYN1PNhu_8ed3aWr5HalNg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1VSyg8f5MV5OE1NQat7SwpA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1pxGOcFSoX3CchIBEM2oqcw">300d</a></td>    </tr>    <tr  align="center">      <td>Weibo 微博</td>      <td><a href="https://pan.baidu.com/s/1gGRPYcczvEEZhxhwjQ43uA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1tGhA_7KGw8KM1puXPoIXSA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1__HAjNrTaFRtZoD1U0EUow">300d</a></td>      <td><a href="https://pan.baidu.com/s/1HHhnqLT_AbIZjE-uoSaSPw">300d</a></td>    </tr>    <tr  align="center">      <td>Literature 文学作品</td>      <td><a href="https://pan.baidu.com/s/13qMvNLzjOSjorpHWirrFfQ">300d</a></td>      <td><a href="https://pan.baidu.com/s/1H3xUanO1gUnkyBqybtrHVw">300d</a></td>      <td><a href="https://pan.baidu.com/s/1yS7546XiINmRvLgDQKmeWA">300d</a></td>      <td><a href="https://pan.baidu.com/s/1sVXgEjUGEocQfMHNf8-TQQ">300d</a></td>    </tr>    <tr  align="center">      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>      <td><a href="https://pan.baidu.com/s/1KMpEAWhmItv5zpwCstcRQg">300d</a></td>      <td><a href="https://pan.baidu.com/s/1xl15aCTVBow0iJkxjDeykQ">300d</a></td>      <td>NAN</td>      <td>NAN</td>    </tr>    </tr>    <tr  align="center">      <td>Mixed-large 综合</td>      <td>300d</td>      <td>300d</td>      <td>300d</td>      <td>300d</td>    </tr></table>




### 词共现性


我们发布的词向量来自不同的共现统计量。目标和上下文向量在一些相关论文中通常被称为输入和输出向量。

在这一部分中，可以获得超出单词的任意语言单位的向量。例如，字符向量(word-character)。

所有向量均由SGNS在百度百科上进行训练。

<table>  <tr align="center">    <td><b>Feature</b></td>    <td><b>Co-occurrence Type</b></td>    <td><b>Target Word Vectors</b></td>    <td><b>Context Word Vectors</b></td>  </tr>  
  <tr align="center">  	<td rowspan="1">Word</td>    <td>Word → Word</td>    <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/1oVCCc_GNMdGKJHXB2dS2LQ">300d</a></td>  </tr>
  <tr align="center">    <td rowspan="3">Ngram</td>    <td>Word → Ngram (1-2)</td>    <td><a href="https://pan.baidu.com/s/1SC2p5SkMHQNFuTf21Xcx-Q">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/108isvxCIodOKLcixUVDHqA">300d</a></td>  </tr>  <!-- 以下网盘没有备份  -->  <tr align="center">    <td>Word → Ngram (1-3)</td>    <td><a href="https://pan.baidu.com/s/1oUmbxsnSuXf2jU8Jxu7U8A">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/1ylg6FfFHa0kXbiVz8bIL8g">300d</a></td>  </tr>  <tr align="center">    <td>Ngram (1-2) → Ngram (1-2)</td>    <td><a href="https://pan.baidu.com/s/1Za7DIGVhE6dMsTmxHb-izg">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/1oKI4Cs9eo7bg5mqfY1hdmg">300d</a></td>  </tr>  
  <tr align="center">    <td rowspan="3">Character</td>    <td>Word → Character (1)</td> 	  <td><a href="https://pan.baidu.com/s/1c9yiosHKNIZwRlLzD_F1ig">300d</a></td>    <td><a href="https://pan.baidu.com/s/1KGZ_x8r-lq-AuElLCSVzvQ">300d</a></td>  </tr>  <tr align="center">    <td>Word → Character (1-2)</td> 	  <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>    <td><a href="https://pan.baidu.com/s/1q0ItLzbn5Tfb3LhepRCeEA">300d</a></td>  </tr>  <tr align="center">    <td>Word → Character (1-4)</td>    <td><a href="https://pan.baidu.com/s/1WNWAnba56Rqjmx-FAN_7_g">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/1hJKTAz6PwS7wmz9wQgmYeg">300d</a></td>  </tr>  
  <tr align="center">  	<td rowspan="1">Radical</td>    <td>Radical</td>    <td>300d</td> 	  <td>300d</td>  </tr>  
  <tr align="center">    <td rowspan="2">Position</td>    <td>Word → Word (left/right)</td>    <td><a href="https://pan.baidu.com/s/1JvjcrXFZPknT5H5Xw6KRVg">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/1m6K9CnIIS8FrQZdDuF6hPQ">300d</a></td>  </tr>  <tr align="center">    <td>Word → Word (distance)</td>    <td><a href="https://pan.baidu.com/s/1c29BDu4R1hyUX-sgvlHJnA">300d</a></td> 	  <td><a href="https://pan.baidu.com/s/1sMZHIc-7eU6gRalHwtBHZw">300d</a></td>  </tr>  
  <tr align="center">    <td>Global</td>    <td>Word → Text</td>    <td>300d</td> 	  <td>300d</td>  </tr>    
  <tr align="center">    <td rowspan="2">Syntactic Feature</td>    <td>Word → POS</td>    <td>300d</td> 	  <td>300d</td>  </tr>  <tr align="center">    <td>Word → Dependency</td>    <td>300d</td> 	  <td>300d</td>  </tr></table>
## 表示
现有的单词表示方法分为两类，即**密集**和**稀疏**向量。 SGNS模型（word2vec工具包中的一个模型）和PPMI模型分别是这两类的典型方法。 SGNS模型通过浅层神经网络训练低维实数（密集）向量。它也被称为神经嵌入方法。 PPMI模型是一种稀疏的特征表示，通过PPM加权方案进行加权。

## 上下文特征


三个上下文特征：** word **，** ngram **和** character **常见于文献中。大多数单词表示方法主要利用单词共现统计，即使用单词作为上下文特征**（单词特征）**。受语言建模问题的启发，我们在上下文中引入了ngram特性。 word-word和word-ngram共现统计都用于训练**（ngram特征）**。对于中国人来说，汉字（汉字）通常表达强烈的语义。为此，我们考虑使用单词和单词字符共现统计来学习单词向量。字符级ngram的长度范围从1到4 **（字符特征）**。


除了word，ngram和character之外，还有其他对单词向量的属性具有实质影响的特征。例如，使用整个文本作为上下文特征可以将更多的主题信息引入词向量;使用依赖关系解析作为上下文特征可以为词向量添加语法约束。本项目考虑了17种同现类型。



## 语料库
我们费了很大劲收集各个领域的语料。所有文本数据都通过删除html和xml标签进行预处理。只保留纯文本，并且[HanLP（v_1.5.3）]（https://github.com/hankcs/HanLP）用于分词。详细的语料库信息如下所示：


<table>	<tr align="center">		<td><b>Corpus</b></td>		<td><b>Size</b></td>		<td><b>Tokens</b></td>		<td><b>Vocabulary Size</b></td>		<td><b>Description</b></td>	</tr>	<tr align="center">		<td>Baidu Encyclopedia<br />百度百科</td>		<td>4.1G</td>		<td>745M</td>		<td>5422K</td>		<td>Chinese Encyclopedia data from<br />https://baike.baidu.com/</td>	</tr>	<tr align="center">		<td>Wikipedia_zh<br />中文维基百科</td>		<td>1.3G</td>		<td>223M</td>		<td>2129K</td>		<td>Chinese Wikipedia data from<br />https://dumps.wikimedia.org/</td>	</tr>	<tr align="center">		<td>People's Daily News<br />人民日报</td>		<td>3.9G</td>		<td>668M</td>		<td>1664K</td>		<td>News data from People's Daily(1946-2017)<br />http://data.people.com.cn/</td>	</tr>	<tr align="center">		<td>Sogou News<br />搜狗新闻</td>		<td>3.7G</td>		<td>649M</td>		<td>1226K</td>		<td>News data provided by Sogou labs<br />http://www.sogou.com/labs/</td>	</tr>  <tr align="center">    <td>Financial News<br />金融新闻</td>    <td>6.2G</td>    <td>1055M</td>    <td>2785K</td>    <td>Financial news collected from multiple news websites</td>  </tr>	<tr align="center">		<td>Zhihu_QA<br />知乎问答</td>		<td>2.1G</td>		<td>384M</td>		<td>1117K</td>		<td>Chinese QA data from<br />https://www.zhihu.com/</td>	</tr>	<tr align="center">		<td>Weibo<br />微博</td>		<td>0.73G</td>		<td>136M</td>		<td>850K</td>		<td>Chinese microblog data provided by NLPIR Lab<br />http://www.nlpir.org/download/weibo.7z</td>	</tr>	<tr align="center">		<td>Literature<br />文学作品</td>		<td>0.93G</td>		<td>177M</td>		<td>702K</td>		<td>8599 modern Chinese literature works</td>	</tr>	<tr align="center">		<td>Mixed-large<br />综合</td>		<td>22.6G</td>    <td>4037M</td>    <td>10653K</td>		<td>We build the large corpus by merging the above corpora.</td>	</tr>  <tr align="center">    <td>Complete Library in Four Sections<br />四库全书</td>    <td>1.5G</td>    <td>714M</td>    <td>21.8K</td>    <td>The largest collection of texts in pre-modern China.</td>  </tr></table>
所有的单词都考虑在内，包括低频词。



## 工具包

所有的单词向量由[ngram2vec]（https://github.com/zhezhaoa/ngram2vec/）工具箱进行训练。 Ngram2vec工具箱是[word2vec]（https://github.com/svn2github/word2vec）和[fasttext]（https://github.com/facebookresearch/fastText）工具箱的超集，支持任意上下文特性和模型。

## 中文推理基准

词向量的质量通常通过推理问题任务来评估。在这个项目中，有两个基准被用于评估。首先是CA翻译，其中大多数类比问题直接从英语基准转换而来。虽然CA翻译已被广泛用于许多中文word embedding论文，但它只包含三个语义问题的问题，涵盖了134个中文单词。相比之下，CA8是专门为中文而设计的。它包含了17813个类比问题，涵盖了全面的形态和语义关系。 CA转换后的CA8及其详细描述在[** testsets **]（https://github.com/Embedding/Chinese-Word-Vectors/tree/master/testsets）文件夹中提供。

## 评估工具包
我们在[**evaluation**]（https://github.com/Embedding/Chinese-Word-Vectors/tree/master/evaluation）文件夹中提供评估工具包。

运行以下代码来评估密集向量。
```
`$ python ana_eval_dense.py -v <vector.txt> -a CA8/morphological.txt
$` python ana_eval_dense.py -v <vector.txt> -a CA8/semantic.txt
```
运行以下代码来评估稀疏向量。
```
`$ python ana_eval_sparse.py -v <vector.txt> -a CA8/morphological.txt
$`


> **注意**
> 本文由jupyter notebook转换而来, 您可以在这里下载[notebook](最全中文词向量数据下载-都是训练好的优质向量.ipynb)
> 有问题可以直接在下方留言
> 或者给我发邮件675495787[at]qq.com
> 请记住我的网址: mlln.cn 或者 jupyter.cn
