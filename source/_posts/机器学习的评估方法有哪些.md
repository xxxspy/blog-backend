---
title: 机器学习的评估方法有哪些
date: 2018-10-13 14:11:44
tags: [机器学习]
---

常见的模型评估方法有以下三种：

<!-- more -->

### 一、留出法

留出法将样本D划分成两部分：训练集和测试集

1.划分时一般不宜随机划分，因为如果T中正好只取到某一种特殊类型数据，从而带来了额外的误差。此时处理方法要视具体情况而定，如当数据明显的分为有限类时，可以采用分层抽样方式选择测试数据，保证数据分布比例的平衡。

### 2. 留出法带来一个无法避免的矛盾：

我们初始动机是“评估数据集D训练出的模型”但是我们把数据集分开了，导致：

Ⅰ 若 S较大T较小，那么S训练出的模型与D训练的模型相似，但是T太少，评估结果偶然性大，不准确。

Ⅱ 若S较小T较大，那么S与D训练出的模型差异较大，T的评估失去意义。

这种矛盾是无法避免的。常用做法是选择1/5-1/3左右数据用于评估。

二、交叉验证法

一个交叉验证将样本数据集分成两个互补的子集，一个子集用于训练（分类器或模型）称为训练集（training set）；另一个子集用于验证（分类器或模型的）分析的有效性称为测试集（testing set）。利用测试集来测试训练得到的分类器或模型，以此作为分类器或模型的性能指标。得到高度预测精确度和低的预测误差，是研究的期望。为了减少交叉验证结果的可变性，对一个样本数据集进行多次不同的划分，得到不同的互补子集，进行多次交叉验证。取多次验证的平均值作为验证结果。

常见类型的交叉验证：

1、重复随机子抽样验证。将数据集随机的划分为训练集和测试集。对每一个划分，用训练集训练分类器或模型，用测试集评估预测的精确度。进行多次划分，用均值来表示效能。

优点：与k倍交叉验证相比，这种方法的与k无关。

缺点：有些数据可能从未做过训练或测试数据；而有些数据不止一次选为训练或测试数据。

2、K倍交叉验证（K>=2）。将样本数据集随机划分为K个子集（一般是均分），将一个子集数据作为测试集，其余的K-1组子集作为训练集；将K个子集轮流作为测试集，重复上述过程，这样得到了K个分类器或模型，并利用测试集得到了K个分类器或模型的分类准确率。用K个分类准确率的平均值作为分类器或模型的性能指标。10-倍交叉证实是比较常用的。

优点：每一个样本数据都即被用作训练数据，也被用作测试数据。避免的过度学习和欠学习状态的发生，得到的结果比较具有说服力。

3、留一法交叉验证。假设样本数据集中有N个样本数据。将每个样本单独作为测试集，其余N-1个样本作为训练集，这样得到了N个分类器或模型，用这N个分类器或模型的分类准确率的平均数作为此分类器的性能指标。

优点：每一个分类器或模型都是用几乎所有的样本来训练模型，最接近样本，这样评估所得的结果比较可靠。实验没有随机因素，整个过程是可重复的。

缺点：计算成本高，当N非常大时，计算耗时。

训练集和测试集的选取：

1、训练集中样本数量要足够多，一般至少大于总样本数的50%。

2、训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。（随机取样，可重复性差）

### 三、bootstrap方法（自助法）

bootstraping是对于原来有m个元素的样本集进行m次随机采样，从而生成一个和原来样本集一样大的训练集，可知原集合里未被抽到的元素概率可用p=（1-1/m）^m 表示，根据求极限原理当m趋近无穷大，p=1/e （0.368）。也就是说通过自主法我们可以获得一个和原集合一样大的训练集（由于随机抽样我们认为这个训练集的元素特征与原集合相似）和一个三分之一大小与之完全互斥的测试集。对于小数据集这提高了了训练和测试的性能，但如果数据集过大反而会造成较大的数据遗漏。





在进行机器学习模型得评估时，要根据现在数据得类型、质量、数量等等因素选择合适得模型评估方法，进而评估模型。
